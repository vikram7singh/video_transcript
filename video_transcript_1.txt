 I'm talking about Go and SQLite. I'm going to talk about it in a few different ways. First off, I worked at Google for eight years. I was on the Go team. I did a few other things there. And I left a few months ago. And now I'm doing my own thing, I guess, which just means programming on my own time. Anyway, along the way at Google, I learned a lot of things. Some of them good. Actually, a lot of them good. I learned a ton about programming. And a ton about teams and how they work. And I want to bring those things with me into my new work. Make sure that things I learned about code review, about testing, about how to deploy systems, about how to monitor systems, and make sure they're working, how to canary out new builds, and see how things work. All of these are great techniques. And I want to use them all. But there's a problem, which is that when you're at a big tech company and you have a bunch of engineering problems, and you make a list of the engineering problems you have to solve to launch a product, you then you go and solve them. And if you can't solve them, you just go and find more engineers, and you get them to solve them. And if you want to be cynical about this, you can use the Futurama meme about kill bots and use through wave after wave of engineer the problem until it's solved. But that's cynical. And the truth is, if you have a lot of money and you have a big problem to solve, you use the money to solve the problem. It's completely reasonable. But it doesn't work for me because I don't have all the money. And so I have to just solve it myself. And so now I'm between a rock and a hard place. Like, I need to build the product, but I only have these tools. That's what I do. So I've been going through from the ground up every single thing I've ever used, and deciding what I can keep and what I have to throw out, just because I don't have time. And what I've come up with is go and ask you light as sort of the two key components that I'm keeping or reintroducing in the case of SQLite, because they don't get used much to big tech companies, at least not in the cloud. So this is all across a part-time project. This is what I spend most of my time doing. Let's take a look at this guy. I thought I just needed an image of my talk, and you know, he's cute. So, SQLite or SQLite or SQLite, you can call it whatever you want. I'm going to try and rotate through all the different ways of pronouncing it through this talk, keep everyone on the toes, entertaining. It's a database. It's a relational database management system as they used to call them in the 80s. And it's distinguishing feature. It's not unique anymore, but it's distinguishing features. It's a self-contained C library that keeps the entire database in a file on a disk in a file system. Really old school ideas. And it's great. It's so wonderful. Everything about it's terrific. And if you've ever used DB3, it's just like DB3, except good. If you've never used DB3, I've heard of it. Great. And as I said, files, SQLite, starts with this is the biggest assumption. There's a Unix file system or something, Unix-ish. Windows files, something. We can put a file on, you can edit it. And maybe you can even lock it and tell other people it's locked, maybe, maybe sometimes. It's a good assumption. And it's one we throw out a lot. And I would like us to throw it out less. And that's one of the things I will be talking about. It ends this talk. But instead of talking, let's do a demo. Because demo is fun. Let's, at least let's see if I can do a demo. I don't think I've ever done a live coding demo. And I'm going to do some data science. I think they call it, I don't know, find out. I have a file, I stole off Kaggle. Kaggle is really good, by the way. And if you take a look at it, it's Shakespeare. It's Shakespeare line by line. Line by line. All of it. There's not actually a lot of it. It's about 10 megabytes of data. Would have been a big nuance. So we're going to put it in SQLite. We're going to do a couple things to it to see how it works. So let's give it a go. So we're going to create an SQLite database. Done. All right. Now let's use some special tools. This is the SQLite command line utility. It comes from Richard Hippey made SQLite on the SQLite.org website. And it's got a couple of non-SQL. Let me back up. Who here is used SQL? Great. That's everybody. Who here is used SQLite? That's everybody. So I can just skip all of this. Yeah. No, it's wonderful. It's been around nearly 20 years now. And it's just one of those ubiquitous components programming for you. It's right here in my Mac. I didn't install this. This is the binary app or ship to me. It's wonderful. It's on everyone's Android phone. It's on everyone's iPhone. It's in space. If you can think of a place they put it there, it's in Airbus control software for the flight control software. I'll get to that in a second. So anyway, I don't think this bit is. And hopefully the Shakespeare isn't. Shakespeare is good, by the way. He's in that very awkward place of being both excellent and overrated at the same time. You can blame the Victorians for that. Back in the 19th century. We can blame them for just about everything. But he's still really excellent. So you should read them. Or you should put them in your SQLite database. So we've not now SQLite command line utility in the CSV mode. And we're going to import that CSV file. It's got some nice tab completion, which is good, because typing in front of a crowd is really something. We're going to put it in a table called import. OK, we've done that. If we take a look, it created a table named import. And it picked the first line of the CSV and used that for the column names. And we could take a quick look at that. Like this. Just look at the first 10 rows. Yeah, that looks about right. That's basically what we saw. And we took a look at the top of the file. We can count that and make sure it makes sense. Yeah, it's about 100,000 lines. OK, so now let's manipulate that data a little bit. And then do something with it. So let's move it all into a new table called plays. And we'll have a row ID on that table. I'll get it to your primary key. And here's something addressing that SQLite. That is me giving a type to a column name, row ID, and specifying it's the primary key of the table. But all these other columns, I'm not even going to bother doing that because SQL's type system is terrible. And SQLite did the sensible thing, which is to ignore it, which is wonderful. So now we've got a new table here. And if we just look at the schema again, you see we've got this new table I created. So now let's copy all the data from one into the other. And we'll go insert into plays. And you insert select syntax, which is great fun. So this is just the output of the select goes into the other table. So we're going to take the row. We're going to select, oh, it's not called row anymore. Oh, look at this. My notes are out of date. We're going to select data line as row ID. And I wonder if we have to convert it to an integer. We'll just see what happens. What could go wrong? Then we'll take play, we'll take play a line number, and rename that as our line number. And OK, we have this act scene line thing, which if you looked earlier, if you were paying attention, I hope you weren't, because it's too much data. It's a number dot, number dot, sort of like a sem bar value. And it's act scene line. Let's split those out in the columns. And let's do like a semi-competent job of that. I believe there's a bug in this, which I will let you to read a find and fix. So what I'm doing here is I'm treating act scene lines as a string, and I'm slicing into it and taking out the first character as the field act. I'm going to take out the third character, which is after the dot, as a scene, and then I'll take the rest as, so after the second dot, I'm going to take the next five, say as a line number. I missed a what? I missed an as? I did miss an as? Thank you. That is the kind of heckling I can really appreciate. And a line as text, and then we have to say, where we're coming from? Import. Let's see if it worked. Oh, it didn't fail. It's off from, and then plays, limit 10. That looks about right. And look, we have a whole bunch of numeric fields now where we had the string field earlier. Wonderful. So now we can do some, let's look around our Shakespeare. We have famous solidic we that we all know from school. By a guy who liked to procrastinate, procrastination never ends well, is the lesson. At least it should be. He rattles on on a roof for a little while about what to do. I'm pretty sure he said that, but I can't find it. So this is probably like a capitalization issue, or there's going to be some punctuation here or something. So let's improve our search of that a little bit. Because what I really want is to search like I'm using Google or something. And what I'm actually using here is a string matching thing. So that's no good. So let's just do some full text search engine stuff, because that's always great fun. So we'll create a virtual table. Play search using FTS5. How's this? Play. Play is a raw IP. And text is a new table. And now we're going to insert into that from our play table. And we'll just copy in the row ID and the actual text that we'll search over rather than everything from the play table. You could just use this as your main table and skip that. But I will keep them separate. And I'll explain why I do that in a little bit. Can I move them up? Is that better? And speak up. Oh, yeah, I have a mumbling problem. Thank you. So insert into play. Search select. We're doing an insert select. Let's try that. OK, so now we've built a full text search index that we can take a look around in. So where earlier we went for select staff from play, is let's instead give a select staff from play search. Where text, and instead of like we use the match keyword, there it is. OK, so there was a capital W. And tears, of course, has a post tree at the beginning of it. That makes sense. All right, and it's on row ID, 34,231, which is everyone who remembers their Shakespeare windows. So we're done. We've got a full text search. And all of this is just in the default, but I didn't rebuild this or anything. Everything just, you have a full text search system just sitting on your machine that you can use trivially like this. And just to show off some SQL, let me see if I can type this out. So let's try and do a join and get all of accessing a field of the table and the field names are overloaded. So I'm using a qualifier. So we're going to use, we're going to do the previous query we did in play search. And we're going to do an inner join, which is the only join I can ever remember. And it's the join when you go one to one with another table. So we're going to inner join the plays table. And we're going to do it on play search play row ID equals the plays row ID, which is where it came from. If you remember my selected search from earlier, and then we're going to put our work laws back on this, where text match whether it is no in the mind. And a big, you call a name text. So this one here I didn't qualify. There's both tables have a text, which one do you mean? I mean the one in the full text search index. OK, there we go. So Act 3, C1, line 65. How much says that? Yes, he did. Great. So that's roughly, that's SQLite. There's everything you need to know. Wonderful. If we remove that first import table like created, and we clean up after ourselves, and then we exit, you'll see our database is about twice the size of our common separated value file. You remember our database has two copies of all text in it, because there's a full text search index, any original plays index. So it's a really efficient file format too. It's actually pretty good. So that's SQLite. That's really everything you need to know. That wasn't the worst thing that could have ever happened that demo. So let's declare victory. Let me check my time. A little bit over, but that's all right. I'll cut stuff. So SQLite. You can use it from Go. There's a SQL driver. There's actually several SQL drivers. If you look around, if you go to GoDoc.org, it's very confusing. This one's pretty good. This has been around for a very long time, and does what you need. You can use the database SQL package. You can use SQLite. And it looks kind of like this. And this is that join I just did. Yes, I prepared some notes from my live demo. Written in Go, and this does all the work. It opens up the database. It does the join. It uses query row, which uses reflect onto the hood to grab all of the fields out into values, and then prints them back out for you. And so if you run this, you'll see basically what you saw when I ran our last select query by hand. And that's all you really need to get started to be using this stuff. But there's also some other fun things SQLite does, which I'm going to talk about during this talk. And that CIGA wrapper, none of the others support them. So I wrote one. It's not a database SQL driver. It probably should be at this point, and so I'll get around to writing that one day. But it's actually just as close a CIGA, as close a wrapper as possible to the SQLite API in Go. And the key things it uses that I'll mention here are streaming blobs, session extension, and shared cache. It has a few other neat tricks, but that's a big one. So this is a great time to have an aside about CIGA, which I was told to have CIGA and I are great old friends. I've used it for many years, for many things. Everyone hates it. It's for reasons. Pretty good reasons, really. The usual reason people hate it is, oh, I just brought some C code into my Go code. And my C code is terrible. And that's true. Most C code is terrible. And you're much better off if you rewrite your C code and go as a good general rule. That rule doesn't apply here. And the basic reason it doesn't apply is that when we use programming languages to build programs, programming languages have all these features. And the features help us be more productive. They help us work better in teams. And they help us write correct code without bugs. You're not writing SQLite just using it. You don't have to worry about the SQLite team and how they're building this C code. You don't have to worry about their productivity either, somehow they build an entire database into a C file and give it away. And when it comes to bugs, all of the tools we use in programming language, I'm sorry, to avoid bugs, shortcut ways of testing. So types, all these things that we use, are just us getting around the fact that we need to run the program and try it and see if it works in our actual environment. SQLite has all the tests done. They tested everything. It's done. It's completely correct. It's not. It's got bugs. But it's the most tested program that you can download on the internet, I claim. I think people, there are a few things that have better testing than SQLite, but they mostly live in avionics somewhere, and they're much smaller than SQLite. It's just, and there's a wonderful document about this on the SQLite website. You should take a look at it. It's astonishing. It's the most correct part of your program. You are not compromising your Go program by introducing SQLite into it. Cross compilation is harder, that's true. You have to have a C compiler for your target environment on your local machine, which means you have to install a Linux compiler on your Mac. It's the practice of this. That's annoying. There are packages to help. I can't really play that. Your initial Go build is slow. It used to be every Go build was slow, and you had to remember the dash I and all this stuff and knowing you about that. But that actually got fixed on the last Go release. Your into Media Desk, your light package, doesn't get rebuilt each time. So you only notice that the first time it takes 30 seconds to build on my dinghy little MacBook. And after that, it's one second to get your binary. The actual driver I showed you earlier, unfortunately, wraps some SQLite features as which are hidden behind C Macro's at build time using Go build tags, which is not great. I really don't recommend doing that. You have to get in and modify the drive yourself. It's not fun. I have a branch on my little low-level driver where I'm working on this. I'm waiting on the next Go release, which has a feature I need. There should be some fun stuff there. So where do I use this? I use it in a bunch of places. And that's actually a very important point I'll get to. Simply start with the client. I write iOS apps, MacOS programs, and I think for Windows. All right. I have them in various states of readiness. And I do a lot with this. I have entire copies of all user data on the local device. So I have to do a lot of heavy lifting locally. So this is where that full text search I showed you earlier comes in. SQLite's really quite good to this. It does mean I have to think a lot about cloud syncing, or you could call it backups to the cloud because all of your data is local. And how to deal with multiple clients interacting, which SQLite also helps with, which I'll show you in a moment. But just a audience question. How many people use Go on the client? Yeah, that's not very many. OK, that's very few hands. And who has done client development work and keep your hand up if you use SQLite on the client? OK, so far more people have done client development work and have used Go on the client, which makes sense. And it looks like more people have used SQLite on the client than Go. So I claim there's nothing controversial about using SQLite on an iPhone or on Android app. There is something kind of controversial about using Go for this. It's not very widely done. And again, if you're trying to cut features, you have to worry about like, are you doing something really unusual that's going to lend you in a bunch of bugs? So the big question is, should I be using Go for this at all? I claim I should. I should, because I use Go for my servers. And I have only so much space in my head. And that's a big part of the problem of being one person is I can't just go and learn other things. Or I can, but then something else falls out of my head. And all this consumes hours. And so I need to cut to think down the space of tools I use. And so I use Go on the client. And it works pretty well, actually. So back to the Cloud Sync problem, back to SQLite. There was an extension to SQLite called the session extension. And when you create a database connection, this is the Go API from my little level driver. There's a C API that looks roughly like this. When you create a database connection, you can register, you can create a session on it, and then the session objects sticks around. Then you can use that connection to select insert, update, delete all the usual things you do run transactions. At any point, you can call the change set method on the session and get back a blob, which contains the difference between when you created that session and where you are now on that database. So you can take this blob and you can store it somewhere. You can grab an old copy of the database from some other time and then read the blob back and apply it, the change set apply method. So you can take version one of SQLite database, make some changes, record a patch set on that, and then take another version one, and bring it back to where you are now. So this is how I do client sync and backup. I have a, on the client device, a periodically recorded change set, send it to a server. The server applies it to a copy of the database. It's keeping around. And then I keep a stack of change set applies that can be sent down to update other clients. This is a little bit tricky because as you can see in the change set apply parameters as a conflict function. So you have to be very careful about the way you design your database so that you don't create lots of conflicts. The biggest thing there is row IDs. So row IDs and SQLite as everyone knows are auto increment as you create rows. That just means they can flick all the time. So you need to start randomizing those or you need to start bringing them out of some range that's been reserved for each client. And then of course you need to make sure that you have lots of data that's append only or append only or delete or things like that which minimize your conflicts. It works really well. It's been great fun using that. The, just to go back. The session objects are extremely small. They're very efficient. They're also efficient in the sense that if you update a field in the database from one to two, then two to three, two to four, the change set you get out just has the final jump. It doesn't just record every statement and run them back. So another fun thing I found and this is not as vital but it is a lot of fun is nested transactions. So databases have transactions. They're absolutely vital for doing useful things with databases. You begin a transaction as the SQL term and then you commit one. There's another piece of SQL for creating transactions which is the save point which works exactly like begin and then you release it to commit it. With the one difference that you give it a name and then you can start all the transactions inside it. And then you can roll back those transactions. If you want so here we have a series of nested transactions we roll back the inner ones so it doesn't apply. This fits surprisingly well into a little idiom I have in my Go programs that doesn't fit with the database SQL driver. So it's kind of a sort of fun little invention. And that is at the top of every function I have that's doing transactional work on the database. I start a save point so on the connection I register I give the save point a name and then I defer a function which checks the error value I'm about to return from my Go code. So this function after returns an error and the defer checks and if error is nil then it commits the transaction. If it's not it rolls back the transaction that this little F function was doing. And then at any point during this function for reasons of database access through other functions or through parsing some data and it's incorrect you can just return a whole thing rolls back which is a very lightweight way of thinking about transactions. Most importantly you see here we call G. G can do all of this again. So it at the very top can register a save point and then can defer either releasing or rolling it back. And then you can call G completely out of context. You don't need to have to call it through F. It has its own self-contained transaction as long as it logically makes sense as a transaction. You can do that. And I find that a surprising number of my Go functions actually do make logical sense as a database transaction. And so I do this a lot. The one thing is there's actually quite a lot of bookkeeping to get right inside that defer function because the SQL actions can fail and then you have to create errors for them and return those errors appropriately. It ends up being like 50 lines of the defer block. So I have a little utility function that does this whose syntax looks a little unusual the first time you see it but you get used to it pretty quickly. And so at the very top of each of these transactional functions I call save on the connection which returns a function that takes a pointer to an error and you pass it a pointer to your error and then you defer that. And so you get this sort of gnarly one liner which does all of the work of creating a save point transaction and either releasing or rolling it back based on the error condition of the function you're in. And then you can do this in F and then you can do this in G and you get your nice nested transactions. It does a few other things for you. It picks a name of the save point based on the function you're in and using a walking the call stack which is surprisingly fast. And so you get quite nice error messages from SQLite when you screw up your SQL, telling you exactly where it went wrong. So those are tricks I use on the client. I use that trick on Cloud programming too. So now I want to talk a little bit and this is a little bit controversial so I'll keep it short and vague and tell you it probably doesn't apply to you working at a well-capitalized tech company. I write a server in a process. So I build a process and it's got an HTTP port and an SMTP port and a bunch of other things that registers. And then I run it on a VM in an availability zone on a Cloud hosting provider, one of the big three. And that's it, that's all I do. I overprovision the machine because why not? It cost me like 50 bucks a month which makes it really easy for me to, you know, can area out a new one, divert some of the traffic. I have a little thing for doing that, little thousand line harness that I built. And that's it. And everything lives in this one process and it kind of works. And this one process includes the database. It's got SQLite that it reads a file on the machine. The file is stored on the NAS system. So I'm using Amazon right now. They've been trying the other ones and they call their NAS system EBS. I'm going to go into that a little bit. But the key thing here is this is sort of like one computer programming. So I can scale my service until I hit the limits of one computer and then it's all over, a game over. I can't do anything more, right? Oh no, right, I can't scale. That's one of the things I have to throw away, right? Like I can only do so much as one person. I can't build big distributed things that scale. It's just, I can't do it. Fortunately, I don't need to. Turns out one computer can do an awful lot. Like a lot. I mean these scaling limits are you get 128 CPU threads, you get four terabytes of RAM, you can saturate a 25 gigabit ethernet link. Like that's what I can build. And nothing I've built is coming anywhere near that. Not nowhere near it. Like I can hit thousands of QPS in a moderately complex service on a very small VM, nothing like this. This is many tens of thousands of QPS. With years of hours of yearly downtime, there's some kind of SLA on an availability zone that's much less than multi availability zone and your redundant services. But I claim you can build most moderately large services doing this. In fact, a lot of the moderately large services I worked on at Google could have been built this way. And it's really unfortunate that Google never had a way to let you build moderately large services like this because you could start a small team and get a service built in a launch of products. And it would be pretty good doing this. And then you could rewrite it if it turned out successful or throw it away if it's not. And you would have saved a lot of money. They don't care about money. That's great. But there are other advantages besides money. The other advantages are one programmer can keep the whole thing in their mind. A small team can. They can move really quickly. It's really easy to bring one of these up, bring it down. It doesn't involve a lot of the machinery that you're working with distributed systems does. I know that's extremely ridiculous. Like who wants to write on a computer, but it's kind of fun. And it's really simple and that's really important because there's only so many things I can keep in my mind. So a few technical details about this. SQLite has a right ahead logging mode, which I highly recommend. Most people, I guess, don't know what exists, you can just not SQLite in a right ahead logging. It creates two extra files when you do this. There's like a little wild file after your DB. And what this is is instead of a rollback journal. So with a rollback journal, SQLite modifies your database inline and then writes a rollback instruction to a rollback to the rollback journal, which says how to undo that if something goes terribly wrong. And actually it doesn't do the other order. So if something breaks halfway, it can roll back the transaction. With right ahead logging, all that happens is a transaction is written as sort of a change set onto a well file and then periodically the system flushes the change sets into the database file. This means that you can be writing while other people are reading. So this is very important. SQLite's huge big restriction, which makes cloud programming difficult, is it has one writer at a time. But it can have N readers and now it can have N readers simultaneously with the writer, which means you don't get lots of service lockup. Writing with a single writer requires a lot of care. You have to be really careful not to block in the middle of your live transactions. A lot of those transactions I was using earlier are read transactions, which I have these problems. They defer locking the database until you actually modify it. But no, it's really, it's the trickiest part. I don't access the network and block on something else while you're holding the right lock on the database or else you have nothing to as well. So storage in the cloud, I mentioned, I use Amazon's NAS system. They warn you it loses data. The numbers they give you don't make any sense. There's no way it loses that much data. I'm just trying to be really careful about it. They have a great snapshot to Blobsaw system where it snapshots the change in their NAS thing to their big georeadundent blob storage system really quickly. And you can program this. And I do. So I manually flush the well log of SQLite, the main one that sits on its own EBS volume. I have a package called really fsync, which triggers the s3 backup. So you just have to think about this in layers, right? Like the fsync on your nice EBS based volume is better than your standard computer at home because it's a nice, rate array across the network. But not as good as it could be. So you have really fsync as your super backup system. And then you have a real backup system which gets it out of s3. Get it out of s3. Keep an open mind, of course, about the stuff. As I have the best NAS SLA, their numbers for this is way better than the EBS numbers from Amazon. Google don't even seem to be trying. I couldn't find any of their SLAs. That's fine. They don't believe in this. It's not their game. But let me say be wary of SLAs. Like, just don't believe any of this stuff. Because I started reading their terms and conditions. And like, oh, yeah, we lost all the data for your business that you've been running for 10 years. And it's all gone. Sorry about that. All the backups are ruined. We're going to give you 50% of your build this month. Yeah, that's great. There's no symmetry in these SLAs. It's not ensured. Like, they tell you these big numbers, and they give you a small discount if it goes wrong. So be super wary of that. Also, be super wary of these dear redundant systems. They claim they deal with long tail events to like 50 billion nines or something. There's no way they can do that because it's written in programming. It's a computer. Like, there are bugs in their software. It's not as well tested as SQLite. I knew that for a fact. And what's going to happen one day is it's just going to lose everything because of some software bug. It's like, oh, sorry. And again, their SLA doesn't require them to pay for it. It will be really embarrassing. So anyway, don't trust any of this stuff. It's part of not trusting any of that stuff. Trust the availability zones because we kind of know how those work. They're more reliable than everyone says. You can write a good system on that. I think I am out of time. So I'm going to skip a few things. But I am going to say, here's a slide about the disadvantages of this sort of one process programming I do, which is there are some things you just can't do this way. You can't build a web search engine this way. You need more than four terabytes of RAM to store a public indexes of the internet. You need a lot more. So don't try and do that. There's also other fun things you can't do. If you find one of these problems, it's terrific build distributed system. Great fun. But work really hard not to try and do this first. You can't pretend you have the ultra-higher liability of these year-relevant systems. Pretending is really nice. It's great to pretend we have 59s or whatever it is. There are real problems. Latency to global customers. You have to put this computer somewhere. Speed of light is slow. CDNs are pretty good. You should use them. You can also consider geo-sharing, which is kind of neat. If your customer data is not deeply connected with other customers data, if it can shard well, you can just move customer data closer to them. Just run a few machines. I'll pick one near them for running out. It also makes cloud confidence kind of boring, because they launch all these great products, and you don't need to need them. It's really unfortunate. But the advantage is, and the most important one is ops. Again, my server looks exactly like my client software. If I get working up in the middle of the night, some kind of page thing, I don't have to remember how post-gress QL works, which I read a tutorial on to get it going, but I've forgotten now. It's exactly like the thing I use all day, every day, on my laptop, for my program. And that's really important. Similarly, one process makes monitoring easier. You don't know the database fall over, and the front-end stay up, or any of that stuff. It's all one thing. Front-end backend database. You can do everything with PPROF. You just look at your PPROF output. Whether your service is fast enough. You don't need to think about how the things interact or anything. You've removed a lot of protocols from everything. It's also really easy to bring it up on other hosts. It's easy to copy Google, or Azure, or anything. So yeah, just don't use any computers when one will do. That's what I have to say. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.